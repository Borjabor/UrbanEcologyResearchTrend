{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e9aaad",
   "metadata": {},
   "source": [
    "# Urban Ecology Research Trend Analysis\n",
    "\n",
    "Type: NLP + Time Series + Web Data | Domain: Scientific + environmental | Format: Notebook\n",
    "- Use PubMed or Semantic Scholar API to extract papers on 'urban ecology'.\n",
    "- Track number of publications per year.\n",
    "- Perform keyword frequency and topic modeling.\n",
    "- Map institutions or authors by location \n",
    "    .Use author search and Research Organization Registry (ror.org) to map institutions to locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For future author search\n",
    "\n",
    "AUTHOR_URL = 'https://api.semanticscholar.org/graph/v1/author/batch'\n",
    "AUTHOR_FIELDS = 'affiliations'\n",
    "AUTHOR_ID_JSON = 'author_ids.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "BASE_URL = 'http://api.semanticscholar.org/graph/v1/paper/search/bulk'\n",
    "FIELDS = 'title,year,authors,url'\n",
    "PUBLICATION_TYPES = 'Review,JournalArticle,Study,Book,BookSection'\n",
    "DELAY = 5  # delay between requests to avoid rate limiting\n",
    "RETRY_DELAY = 5  # seconds before retrying on failure\n",
    "OUTPUT_JSONL = 'papers.jsonl'\n",
    "YEAR_RANGE = '2000-'\n",
    "\n",
    "query_list = [\n",
    "    'urban ecology',\n",
    "    'urban biodiversity',\n",
    "    'urban ecosystem',\n",
    "    'urban green spaces',\n",
    "    'urban wildlife',\n",
    "    'urban vegetation',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4835c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Progress trackers for \"urban ecology\" removed.\n",
      "âœ… Progress trackers for \"urban biodiversity\" removed.\n",
      "âœ… Progress trackers for \"urban ecosystem\" removed.\n",
      "âœ… Progress trackers for \"urban green spaces\" removed.\n",
      "âœ… Progress trackers for \"urban wildlife\" removed.\n",
      "âœ… Progress trackers for \"urban vegetation\" removed.\n",
      "âœ… Output file \"papers.jsonl\" removed.\n"
     ]
    }
   ],
   "source": [
    "# Run this to clear the .txt progress trackers\n",
    "\n",
    "for keyword in query_list:\n",
    "    done_file = f'done_{keyword}.txt'\n",
    "    token_file = f'token_{keyword}.txt'\n",
    "    if os.path.exists(done_file):\n",
    "        os.remove(done_file)\n",
    "        print(f'âœ… Progress trackers for \"{keyword}\" removed.')\n",
    "    if os.path.exists(token_file):\n",
    "        os.remove(token_file)\n",
    "        print(f'âœ… Progress trackers for \"{keyword}\" removed.')\n",
    "        \n",
    "os.remove(OUTPUT_JSONL)\n",
    "print(f'âœ… Output file \"{OUTPUT_JSONL}\" removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fa39136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper functions ===\n",
    "# keeps track of keywords and tokens to allow to keep retrieving papers from \n",
    "# where you left off in case of a request/response error\n",
    "def save_token(keyword, token):\n",
    "    with open(f'token_{keyword}.txt', 'w') as f:\n",
    "        f.write(token)\n",
    "\n",
    "def load_token(keyword):\n",
    "    filename = f'token_{keyword}.txt'\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    return None\n",
    "\n",
    "def mark_done(keyword):\n",
    "    with open(f'done_{keyword}.txt', 'w') as f:\n",
    "        f.write('completed')\n",
    "\n",
    "def is_done(keyword):\n",
    "    return os.path.exists(f'done_{keyword}.txt')\n",
    "\n",
    "def delete_token(keyword):\n",
    "    filename = f'token_{keyword}.txt'\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "        \n",
    "def create_df_from_jsonl(jsonl_path):\n",
    "    if os.path.exists(jsonl_path):\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            papers = [json.loads(line) for line in f]\n",
    "            \n",
    "        for paper in papers:\n",
    "            authors = paper.get('authors', [])\n",
    "            paper['author_names'] = [author.get('name') for author in authors if 'name' in author]\n",
    "\n",
    "        df = pd.json_normalize(papers)\n",
    "\n",
    "        if 'paperId' in df.columns:\n",
    "            df = df.drop_duplicates(subset='paperId')\n",
    "            print(f'ğŸ“Œ Deduplicated. Final count: {len(df)} unique papers.')\n",
    "        else:\n",
    "            print('âš ï¸ Warning: No paperId field found to deduplicate.')\n",
    "            \n",
    "        if 'authors' in df.columns:\n",
    "            df = df.drop(columns='authors')\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print('âš ï¸ No data found. Make sure the JSONL file exists.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3efc42f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Processing keyword: \"urban ecology\"\n",
      "â³ Starting fresh for keyword \"urban ecology\"\n",
      "ğŸ“„ Retrieved 1000 papers so far for \"urban ecology\"\n",
      "ğŸ“„ Retrieved 2000 papers so far for \"urban ecology\"\n",
      "ğŸ“„ Retrieved 3000 papers so far for \"urban ecology\"\n",
      "ğŸ“„ Retrieved 4000 papers so far for \"urban ecology\"\n",
      "ğŸ“„ Retrieved 5000 papers so far for \"urban ecology\"\n",
      "ğŸ“„ Retrieved 5795 papers so far for \"urban ecology\"\n",
      "âœ… Completed all pages for \"urban ecology\"\n",
      "\n",
      "ğŸ” Processing keyword: \"urban biodiversity\"\n",
      "â³ Starting fresh for keyword \"urban biodiversity\"\n",
      "ğŸ“„ Retrieved 1000 papers so far for \"urban biodiversity\"\n",
      "ğŸ“„ Retrieved 2000 papers so far for \"urban biodiversity\"\n",
      "ğŸ“„ Retrieved 3000 papers so far for \"urban biodiversity\"\n",
      "ğŸ“„ Retrieved 4000 papers so far for \"urban biodiversity\"\n",
      "ğŸ“„ Retrieved 5000 papers so far for \"urban biodiversity\"\n",
      "ğŸ“„ Retrieved 6000 papers so far for \"urban biodiversity\"\n",
      "ğŸ“„ Retrieved 6512 papers so far for \"urban biodiversity\"\n",
      "âœ… Completed all pages for \"urban biodiversity\"\n",
      "\n",
      "ğŸ” Processing keyword: \"urban ecosystem\"\n",
      "â³ Starting fresh for keyword \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 1000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 2000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 3000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 4000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 5000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 6000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 7000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 8000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 9000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 10000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 11000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 12000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 13000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 14000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 15000 papers so far for \"urban ecosystem\"\n",
      "ğŸ“„ Retrieved 15567 papers so far for \"urban ecosystem\"\n",
      "âœ… Completed all pages for \"urban ecosystem\"\n",
      "\n",
      "ğŸ” Processing keyword: \"urban green spaces\"\n",
      "â³ Starting fresh for keyword \"urban green spaces\"\n",
      "ğŸ“„ Retrieved 1000 papers so far for \"urban green spaces\"\n",
      "ğŸ“„ Retrieved 2000 papers so far for \"urban green spaces\"\n",
      "ğŸ“„ Retrieved 3000 papers so far for \"urban green spaces\"\n",
      "ğŸ“„ Retrieved 4000 papers so far for \"urban green spaces\"\n",
      "ğŸ“„ Retrieved 5000 papers so far for \"urban green spaces\"\n",
      "ğŸ“„ Retrieved 6000 papers so far for \"urban green spaces\"\n",
      "ğŸ“„ Retrieved 7000 papers so far for \"urban green spaces\"\n",
      "ğŸ“„ Retrieved 8000 papers so far for \"urban green spaces\"\n",
      "ğŸ“„ Retrieved 9000 papers so far for \"urban green spaces\"\n",
      "ğŸ“„ Retrieved 10000 papers so far for \"urban green spaces\"\n",
      "ğŸ“„ Retrieved 10498 papers so far for \"urban green spaces\"\n",
      "âœ… Completed all pages for \"urban green spaces\"\n",
      "\n",
      "ğŸ” Processing keyword: \"urban wildlife\"\n",
      "â³ Starting fresh for keyword \"urban wildlife\"\n",
      "ğŸ“„ Retrieved 1000 papers so far for \"urban wildlife\"\n",
      "ğŸ“„ Retrieved 2000 papers so far for \"urban wildlife\"\n",
      "ğŸ“„ Retrieved 3000 papers so far for \"urban wildlife\"\n",
      "ğŸ“„ Retrieved 3166 papers so far for \"urban wildlife\"\n",
      "âœ… Completed all pages for \"urban wildlife\"\n",
      "\n",
      "ğŸ” Processing keyword: \"urban vegetation\"\n",
      "â³ Starting fresh for keyword \"urban vegetation\"\n",
      "ğŸ“„ Retrieved 1000 papers so far for \"urban vegetation\"\n",
      "ğŸ“„ Retrieved 2000 papers so far for \"urban vegetation\"\n",
      "ğŸ“„ Retrieved 3000 papers so far for \"urban vegetation\"\n",
      "ğŸ“„ Retrieved 4000 papers so far for \"urban vegetation\"\n",
      "ğŸ“„ Retrieved 5000 papers so far for \"urban vegetation\"\n",
      "ğŸ“„ Retrieved 6000 papers so far for \"urban vegetation\"\n",
      "ğŸ“„ Retrieved 7000 papers so far for \"urban vegetation\"\n",
      "ğŸ“„ Retrieved 8000 papers so far for \"urban vegetation\"\n",
      "ğŸ“„ Retrieved 9000 papers so far for \"urban vegetation\"\n",
      "ğŸ“„ Retrieved 9952 papers so far for \"urban vegetation\"\n",
      "âœ… Completed all pages for \"urban vegetation\"\n",
      "\n",
      "ğŸ‰ All keywords processed.\n",
      "ğŸ“Œ Deduplicated. Final count: 40060 unique papers.\n"
     ]
    }
   ],
   "source": [
    "# === Main loop over keywords ===\n",
    "\n",
    "for keyword in query_list:\n",
    "    print(f'\\nğŸ” Processing keyword: \"{keyword}\"')\n",
    "\n",
    "    if is_done(keyword):\n",
    "        print(f'âœ… Keyword \"{keyword}\" already completed. Skipping.')\n",
    "        continue\n",
    "\n",
    "    token = load_token(keyword)\n",
    "    if token:\n",
    "        print(f'ğŸ”„ Resuming from saved token for \"{keyword}\": {token}')\n",
    "    else:\n",
    "        print(f'â³ Starting fresh for keyword \"{keyword}\"')\n",
    "\n",
    "    retrieved = 0\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            'query': keyword,\n",
    "            'fields': FIELDS,\n",
    "            'publicationTypes': PUBLICATION_TYPES,\n",
    "            'limit': 1000\n",
    "        }\n",
    "        if YEAR_RANGE:\n",
    "            params['year'] = YEAR_RANGE\n",
    "        if token:\n",
    "            params['token'] = token\n",
    "\n",
    "        # Retry logic\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                response = requests.get(BASE_URL, params=params, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f'âŒ Request error on attempt {attempt+1} for \"{keyword}\": {e}')\n",
    "                if attempt == 0:\n",
    "                    print(f'â³ Retrying after {RETRY_DELAY} seconds...')\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print('âš ï¸ Skipping this batch due to repeated failure.')\n",
    "                    data = None\n",
    "\n",
    "        if data is None:\n",
    "            print(f'âš ï¸ No data retrieved for keyword \"{keyword}\", breaking loop.')\n",
    "            break\n",
    "\n",
    "        papers = data.get('data', [])\n",
    "        if not papers:\n",
    "            print(f'âš ï¸ No papers returned, assuming end of results for \"{keyword}\"')\n",
    "            break\n",
    "\n",
    "        retrieved += len(papers)\n",
    "        print(f'ğŸ“„ Retrieved {retrieved} papers so far for \"{keyword}\"')\n",
    "\n",
    "        with open(OUTPUT_JSONL, 'a', encoding='utf-8') as f:\n",
    "            for paper in papers:\n",
    "                paper['search_keyword'] = keyword\n",
    "                json.dump(paper, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        token = data.get('token')\n",
    "        if token:\n",
    "            token = data['token']\n",
    "            save_token(keyword, token)\n",
    "            time.sleep(DELAY)\n",
    "        else:\n",
    "            print(f'âœ… Completed all pages for \"{keyword}\"')\n",
    "            delete_token(keyword)\n",
    "            mark_done(keyword)\n",
    "            break\n",
    "\n",
    "print('\\nğŸ‰ All keywords processed.')\n",
    "\n",
    "df = create_df_from_jsonl(OUTPUT_JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cb32e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "search_keyword",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "a7a8c5e5-f56a-49b0-b29b-6ba432e54eb5",
       "rows": [
        [
         "urban ecosystem",
         "12129"
        ],
        [
         "urban green spaces",
         "8150"
        ],
        [
         "urban vegetation",
         "6202"
        ],
        [
         "urban biodiversity",
         "5904"
        ],
        [
         "urban ecology",
         "5795"
        ],
        [
         "urban wildlife",
         "1880"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 6
       }
      },
      "text/plain": [
       "search_keyword\n",
       "urban ecosystem       12129\n",
       "urban green spaces     8150\n",
       "urban vegetation       6202\n",
       "urban biodiversity     5904\n",
       "urban ecology          5795\n",
       "urban wildlife         1880\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['search_keyword'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa91fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
