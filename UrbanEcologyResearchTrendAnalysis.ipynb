{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e9aaad",
   "metadata": {},
   "source": [
    "# Urban Ecology Research Trend Analysis\n",
    "\n",
    "Type: NLP + Time Series + Web Data | Domain: Scientific + environmental | Format: Notebook\n",
    "- Use PubMed or Semantic Scholar API to extract papers on 'urban ecology'.\n",
    "- Track number of publications per year.\n",
    "- Perform keyword frequency and topic modeling.\n",
    "- Map institutions or authors by location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "BASE_URL = 'http://api.semanticscholar.org/graph/v1/paper/search/bulk'\n",
    "FIELDS = 'title,year,authors,url'\n",
    "PUBLICATION_TYPES = 'Review,JournalArticle,Study,Book,BookSection'\n",
    "DELAY = 5  # delay between requests to avoid rate limiting\n",
    "RETRY_DELAY = 5  # seconds before retrying on failure\n",
    "OUTPUT_JSONL = 'papers.jsonl'\n",
    "YEAR_RANGE = '2000-'\n",
    "\n",
    "query_list = [\n",
    "    'urban ecology',\n",
    "    'urban biodiversity',\n",
    "    'urban ecosystem',\n",
    "    'urban green spaces',\n",
    "    'urban wildlife',\n",
    "    'urban vegetation',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4835c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Progress trackers for \"urban ecology\" removed.\n",
      "✅ Progress trackers for \"urban biodiversity\" removed.\n",
      "✅ Progress trackers for \"urban ecosystem\" removed.\n",
      "✅ Progress trackers for \"urban green spaces\" removed.\n",
      "✅ Progress trackers for \"urban wildlife\" removed.\n",
      "✅ Progress trackers for \"urban vegetation\" removed.\n",
      "✅ Output file \"papers.jsonl\" removed.\n"
     ]
    }
   ],
   "source": [
    "# Run this to clear the .txt progress trackers\n",
    "\n",
    "for keyword in query_list:\n",
    "    done_file = f'done_{keyword}.txt'\n",
    "    token_file = f'token_{keyword}.txt'\n",
    "    if os.path.exists(done_file):\n",
    "        os.remove(done_file)\n",
    "        print(f'✅ Progress trackers for \"{keyword}\" removed.')\n",
    "    if os.path.exists(token_file):\n",
    "        os.remove(token_file)\n",
    "        print(f'✅ Progress trackers for \"{keyword}\" removed.')\n",
    "        \n",
    "os.remove(OUTPUT_JSONL)\n",
    "print(f'✅ Output file \"{OUTPUT_JSONL}\" removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa39136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper functions ===\n",
    "# keeps track of keywords and tokens to allow to keep retrieving papers from \n",
    "# where you left off in case of a request/response error\n",
    "def save_token(keyword, token):\n",
    "    with open(f'token_{keyword}.txt', 'w') as f:\n",
    "        f.write(token)\n",
    "\n",
    "def load_token(keyword):\n",
    "    filename = f'token_{keyword}.txt'\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    return None\n",
    "\n",
    "def mark_done(keyword):\n",
    "    with open(f'done_{keyword}.txt', 'w') as f:\n",
    "        f.write('completed')\n",
    "\n",
    "def is_done(keyword):\n",
    "    return os.path.exists(f'done_{keyword}.txt')\n",
    "\n",
    "def delete_token(keyword):\n",
    "    filename = f'token_{keyword}.txt'\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "        \n",
    "def create_df_from_jsonl(jsonl_path):\n",
    "    if os.path.exists(jsonl_path):\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            papers = [json.loads(line) for line in f]\n",
    "            \n",
    "        for paper in papers:\n",
    "            authors = paper.get('authors', [])\n",
    "            paper['author_names'] = [author.get('name') for author in authors if 'name' in author]\n",
    "\n",
    "        df = pd.json_normalize(papers)\n",
    "\n",
    "        if 'paperId' in df.columns:\n",
    "            df = df.drop_duplicates(subset='paperId')\n",
    "            print(f'📌 Deduplicated. Final count: {len(df)} unique papers.')\n",
    "        else:\n",
    "            print('⚠️ Warning: No paperId field found to deduplicate.')\n",
    "            \n",
    "        if 'authors' in df.columns:\n",
    "            df = df.drop(columns='authors')\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print('⚠️ No data found. Make sure the JSONL file exists.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc42f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing keyword: \"urban ecology\"\n",
      "⏳ Starting fresh for keyword \"urban ecology\"\n",
      "📄 Retrieved 1000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 2000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 3000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 4000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 5000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 6000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 7000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 8000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 9000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 10000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 11000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 12000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 13000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 14000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 15000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 16000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 17000 papers so far for \"urban ecology\"\n",
      "📄 Retrieved 17271 papers so far for \"urban ecology\"\n",
      "✅ Completed all pages for \"urban ecology\"\n",
      "\n",
      "🔍 Processing keyword: \"urban biodiversity\"\n",
      "⏳ Starting fresh for keyword \"urban biodiversity\"\n",
      "📄 Retrieved 1000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 2000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 3000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 4000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 5000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 6000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 7000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 8000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 9000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 10000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 11000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 12000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 13000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 14000 papers so far for \"urban biodiversity\"\n",
      "📄 Retrieved 14595 papers so far for \"urban biodiversity\"\n",
      "✅ Completed all pages for \"urban biodiversity\"\n",
      "\n",
      "🔍 Processing keyword: \"urban ecosystem\"\n",
      "⏳ Starting fresh for keyword \"urban ecosystem\"\n",
      "📄 Retrieved 1000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 2000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 3000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 4000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 5000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 6000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 7000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 8000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 9000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 10000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 11000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 12000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 13000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 14000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 15000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 16000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 17000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 18000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 19000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 20000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 21000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 22000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 23000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 24000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 25000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 26000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 27000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 28000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 29000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 30000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 31000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 32000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 33000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 34000 papers so far for \"urban ecosystem\"\n",
      "❌ Request error on attempt 1 for \"urban ecosystem\": HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Read timed out. (read timeout=15)\n",
      "⏳ Retrying after 5 seconds...\n",
      "📄 Retrieved 35000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 36000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 37000 papers so far for \"urban ecosystem\"\n",
      "📄 Retrieved 37177 papers so far for \"urban ecosystem\"\n",
      "✅ Completed all pages for \"urban ecosystem\"\n",
      "\n",
      "🔍 Processing keyword: \"urban green spaces\"\n",
      "⏳ Starting fresh for keyword \"urban green spaces\"\n",
      "📄 Retrieved 1000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 2000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 3000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 4000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 5000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 6000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 7000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 8000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 9000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 10000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 11000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 12000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 13000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 14000 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 14999 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 15999 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 16999 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 17999 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 18999 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 19999 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 20999 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 21999 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 22999 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 23999 papers so far for \"urban green spaces\"\n",
      "📄 Retrieved 24678 papers so far for \"urban green spaces\"\n",
      "✅ Completed all pages for \"urban green spaces\"\n",
      "\n",
      "🔍 Processing keyword: \"urban wildlife\"\n",
      "⏳ Starting fresh for keyword \"urban wildlife\"\n",
      "📄 Retrieved 1000 papers so far for \"urban wildlife\"\n",
      "📄 Retrieved 2000 papers so far for \"urban wildlife\"\n",
      "📄 Retrieved 3000 papers so far for \"urban wildlife\"\n",
      "📄 Retrieved 4000 papers so far for \"urban wildlife\"\n",
      "📄 Retrieved 5000 papers so far for \"urban wildlife\"\n",
      "📄 Retrieved 6000 papers so far for \"urban wildlife\"\n",
      "📄 Retrieved 6651 papers so far for \"urban wildlife\"\n",
      "✅ Completed all pages for \"urban wildlife\"\n",
      "\n",
      "🔍 Processing keyword: \"urban vegetation\"\n",
      "⏳ Starting fresh for keyword \"urban vegetation\"\n",
      "📄 Retrieved 1000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 2000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 3000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 4000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 5000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 6000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 7000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 8000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 9000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 10000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 11000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 12000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 13000 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 13999 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 14999 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 15999 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 16999 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 17999 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 18999 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 19999 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 20999 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 21998 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 22998 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 23998 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 24998 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 25998 papers so far for \"urban vegetation\"\n",
      "📄 Retrieved 25999 papers so far for \"urban vegetation\"\n",
      "✅ Completed all pages for \"urban vegetation\"\n",
      "\n",
      "🎉 All keywords processed.\n",
      "📌 Deduplicated. Final count: 100849 unique papers.\n"
     ]
    }
   ],
   "source": [
    "# === Main loop over keywords ===\n",
    "\n",
    "for keyword in query_list:\n",
    "    print(f'\\n🔍 Processing keyword: \"{keyword}\"')\n",
    "\n",
    "    if is_done(keyword):\n",
    "        print(f'✅ Keyword \"{keyword}\" already completed. Skipping.')\n",
    "        continue\n",
    "\n",
    "    token = load_token(keyword)\n",
    "    if token:\n",
    "        print(f'🔄 Resuming from saved token for \"{keyword}\": {token}')\n",
    "    else:\n",
    "        print(f'⏳ Starting fresh for keyword \"{keyword}\"')\n",
    "\n",
    "    retrieved = 0\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            'query': keyword,\n",
    "            'fields': FIELDS,\n",
    "            'publicationTypes': PUBLICATION_TYPES,\n",
    "            'limit': 1000\n",
    "        }\n",
    "        if YEAR_RANGE:\n",
    "            params['year'] = YEAR_RANGE\n",
    "        if token:\n",
    "            params['token'] = token\n",
    "\n",
    "        # Retry logic\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                response = requests.get(BASE_URL, params=params, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f'❌ Request error on attempt {attempt+1} for \"{keyword}\": {e}')\n",
    "                if attempt == 0:\n",
    "                    print(f'⏳ Retrying after {RETRY_DELAY} seconds...')\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print('⚠️ Skipping this batch due to repeated failure.')\n",
    "                    data = None\n",
    "\n",
    "        if data is None:\n",
    "            print(f'⚠️ No data retrieved for keyword \"{keyword}\", breaking loop.')\n",
    "            break\n",
    "\n",
    "        papers = data.get('data', [])\n",
    "        if not papers:\n",
    "            print(f'⚠️ No papers returned, assuming end of results for \"{keyword}\"')\n",
    "            break\n",
    "\n",
    "        retrieved += len(papers)\n",
    "        print(f'📄 Retrieved {retrieved} papers so far for \"{keyword}\"')\n",
    "\n",
    "        with open(OUTPUT_JSONL, 'a', encoding='utf-8') as f:\n",
    "            for paper in papers:\n",
    "                paper['search_keyword'] = keyword\n",
    "                json.dump(paper, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        token = data.get('token')\n",
    "        if token:\n",
    "            token = data['token']\n",
    "            save_token(keyword, token)\n",
    "            time.sleep(DELAY)\n",
    "        else:\n",
    "            print(f'✅ Completed all pages for \"{keyword}\"')\n",
    "            delete_token(keyword)\n",
    "            mark_done(keyword)\n",
    "            break\n",
    "\n",
    "print('\\n🎉 All keywords processed.')\n",
    "\n",
    "df = create_df_from_jsonl(OUTPUT_JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cb32e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "search_keyword",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e031c2fc-e898-4a3b-b5a1-9e089bda871a",
       "rows": [
        [
         "urban ecosystem",
         "29320"
        ],
        [
         "urban green spaces",
         "19641"
        ],
        [
         "urban vegetation",
         "17385"
        ],
        [
         "urban ecology",
         "17271"
        ],
        [
         "urban biodiversity",
         "13206"
        ],
        [
         "urban wildlife",
         "4026"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 6
       }
      },
      "text/plain": [
       "search_keyword\n",
       "urban ecosystem       29320\n",
       "urban green spaces    19641\n",
       "urban vegetation      17385\n",
       "urban ecology         17271\n",
       "urban biodiversity    13206\n",
       "urban wildlife         4026\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['search_keyword'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa91fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
