{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e9aaad",
   "metadata": {},
   "source": [
    "# Urban Ecology Research Trend Analysis\n",
    "\n",
    "Type: NLP + Time Series + Web Data | Domain: Scientific + environmental | Format: Notebook\n",
    "- Use PubMed or Semantic Scholar API to extract papers on 'urban ecology'.\n",
    "- Track number of publications per year.\n",
    "- Perform keyword frequency and topic modeling.\n",
    "- Map institutions or authors by location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "BASE_URL = 'http://api.semanticscholar.org/graph/v1/paper/search/bulk'\n",
    "FIELDS = 'title,year,authors,url'\n",
    "DELAY = 5  # delay between requests to avoid rate limiting\n",
    "RETRY_DELAY = 5  # seconds before retrying on failure\n",
    "OUTPUT_JSONL = 'papers.jsonl'\n",
    "YEAR_RANGE = '2022-'\n",
    "\n",
    "query_list = [\n",
    "    'urban ecology',\n",
    "    'urban biodiversity',\n",
    "    'urban ecosystem'\n",
    "    'urban green spaces',\n",
    "    'urban wildlife',\n",
    "    'urban vegetation',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4835c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Progress trackers for 'urban ecology' removed.\n",
      "‚úÖ Progress trackers for 'urban biodiversity' removed.\n",
      "‚úÖ Progress trackers for 'urban green spaces' removed.\n",
      "‚úÖ Progress trackers for 'urban wildlife' removed.\n",
      "‚úÖ Progress trackers for 'urban vegetation' removed.\n",
      "‚úÖ Progress trackers for 'urban environmental change' removed.\n",
      "‚úÖ Progress trackers for 'urban landscape ecology' removed.\n",
      "‚úÖ Progress trackers for 'urban ecosystem services' removed.\n"
     ]
    }
   ],
   "source": [
    "# Run this to clear the .txt progress trackers\n",
    "\n",
    "for keyword in query_list:\n",
    "    done_file = f'done_{keyword}.txt'\n",
    "    token_file = f'token_{keyword}.txt'\n",
    "    if os.path.exists(done_file):\n",
    "        os.remove(done_file)\n",
    "        print(f'‚úÖ Progress trackers for \"{keyword}\" removed.')\n",
    "    if os.path.exists(token_file):\n",
    "        os.remove(token_file)\n",
    "        print(f'‚úÖ Progress trackers for \"{keyword}\" removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa39136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper functions ===\n",
    "# keeps track of keywords and tokens to allow to keep retrieving papers from where you left off\n",
    "def save_token(keyword, token):\n",
    "    with open(f'token_{keyword}.txt', 'w') as f:\n",
    "        f.write(token)\n",
    "\n",
    "def load_token(keyword):\n",
    "    filename = f'token_{keyword}.txt'\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    return None\n",
    "\n",
    "def mark_done(keyword):\n",
    "    with open(f'done_{keyword}.txt', 'w') as f:\n",
    "        f.write('completed')\n",
    "\n",
    "def is_done(keyword):\n",
    "    return os.path.exists(f'done_{keyword}.txt')\n",
    "\n",
    "def delete_token(keyword):\n",
    "    filename = f'token_{keyword}.txt'\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "        \n",
    "def create_df_from_jsonl(jsonl_path):\n",
    "    if os.path.exists(jsonl_path):\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            papers = [json.loads(line) for line in f]\n",
    "\n",
    "        df = pd.json_normalize(papers)\n",
    "\n",
    "        if 'paperId' in df.columns:\n",
    "            df = df.drop_duplicates(subset='paperId')\n",
    "            print(f'üìå Deduplicated. Final count: {len(df)} unique papers.')\n",
    "        else:\n",
    "            print('‚ö†Ô∏è Warning: No paperId field found to deduplicate.')\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print('‚ö†Ô∏è No data found. Make sure the JSONL file exists.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc42f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing keyword: \"urban ecology\"\n",
      "‚è≥ Starting fresh for keyword \"urban ecology\"\n",
      "üìÑ Retrieved 1000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 2000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 3000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 4000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 5000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 6000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 7000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 8000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 9000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 10000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 11000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 12000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 13000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 14000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 15000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 16000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 17000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 18000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 19000 papers so far for \"urban ecology\"\n",
      "üìÑ Retrieved 19110 papers so far for \"urban ecology\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[32m     63\u001b[39m     token = data[\u001b[33m'\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[43msave_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     time.sleep(DELAY)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36msave_token\u001b[39m\u001b[34m(keyword, token)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_token\u001b[39m(keyword, token):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtoken_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.txt\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: write() argument must be str, not None"
     ]
    }
   ],
   "source": [
    "# === Main loop over keywords ===\n",
    "\n",
    "for keyword in query_list:\n",
    "    print(f'\\nüîç Processing keyword: \"{keyword}\"')\n",
    "\n",
    "    if is_done(keyword):\n",
    "        print(f'‚úÖ Keyword \"{keyword}\" already completed. Skipping.')\n",
    "        continue\n",
    "\n",
    "    token = load_token(keyword)\n",
    "    if token:\n",
    "        print(f'üîÑ Resuming from saved token for \"{keyword}\": {token}')\n",
    "    else:\n",
    "        print(f'‚è≥ Starting fresh for keyword \"{keyword}\"')\n",
    "\n",
    "    retrieved = 0\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            'query': keyword,\n",
    "            'fields': FIELDS,\n",
    "            'limit': 1000\n",
    "        }\n",
    "        if YEAR_RANGE:\n",
    "            params['year'] = YEAR_RANGE\n",
    "        if token:\n",
    "            params['token'] = token\n",
    "\n",
    "        # Retry logic\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                response = requests.get(BASE_URL, params=params, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f'‚ùå Request error on attempt {attempt+1} for \"{keyword}\": {e}')\n",
    "                if attempt == 0:\n",
    "                    print(f'‚è≥ Retrying after {RETRY_DELAY} seconds...')\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print('‚ö†Ô∏è Skipping this batch due to repeated failure.')\n",
    "                    data = None\n",
    "\n",
    "        if data is None:\n",
    "            print(f'‚ö†Ô∏è No data retrieved for keyword \"{keyword}\", breaking loop.')\n",
    "            break\n",
    "\n",
    "        papers = data.get('data', [])\n",
    "        if not papers:\n",
    "            print(f'‚ö†Ô∏è No papers returned, assuming end of results for \"{keyword}\"')\n",
    "            break\n",
    "\n",
    "        retrieved += len(papers)\n",
    "        print(f'üìÑ Retrieved {retrieved} papers so far for \"{keyword}\"')\n",
    "\n",
    "        with open(OUTPUT_JSONL, 'a', encoding='utf-8') as f:\n",
    "            for paper in papers:\n",
    "                json.dump(paper, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        token = data.get('token')\n",
    "        if token:\n",
    "            token = data['token']\n",
    "            save_token(keyword, token)\n",
    "            time.sleep(DELAY)\n",
    "        else:\n",
    "            print(f'‚úÖ Completed all pages for \"{keyword}\"')\n",
    "            delete_token(keyword)\n",
    "            mark_done(keyword)\n",
    "            break\n",
    "\n",
    "print('\\nüéâ All keywords processed.')\n",
    "\n",
    "df = create_df_from_jsonl(OUTPUT_JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb32e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
